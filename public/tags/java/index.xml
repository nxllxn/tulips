<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Java on Tulips</title>
    <link>https://www.tlst.cc/tags/java/</link>
    <description>Recent content in Java on Tulips</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>Copyright @ 2022 郁金香啊</copyright>
    <lastBuildDate>Wed, 30 Mar 2022 12:44:33 +0800</lastBuildDate><atom:link href="https://www.tlst.cc/tags/java/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Java内存模型</title>
      <link>https://www.tlst.cc/post/java-concurrent-programming-02/</link>
      <pubDate>Wed, 30 Mar 2022 12:44:33 +0800</pubDate>
      
      <guid>https://www.tlst.cc/post/java-concurrent-programming-02/</guid>
      <description>在我们学习操作系统时，有一个章节会讲数据存储相关的的内容，那个经典的金字塔模型。哈哈，又一个Trade off 的完美诠释。这个金字塔模型中指出，通常我们可以很容易地获得一些存储设备，其数据容量大，成本低廉，但是访问速度较慢，比如我们常见的磁盘，甚至可以扩展到网盘。我们还有一些存储介质，其容量非常小，小到当我们使用这类存储介质所能提供的空间时，不得不小心翼翼，而且此类介质一般价格十分高昂，但是由于这类存储介质相对来说更加靠近我们的计算单元，所以其访问速度非常非常快，几乎可以忽略不记，比如我们的寄存器。
在我们讨论这个抽象的金字塔概念之前，让我们先看一个生活中比较常见的例子。比如你是一个木匠，你准备做点科研，造个什么木制品出来，这个作品相当精妙，以至于需要十几种甚至更多的工具来帮助你完成，假定这些工具目前放在你的车库。
现实中，当你用工具的时候你并不会总是去车库找工具，你可能会先找一个小盒子将你认为在不远的将来你可能会用到的工具先一次性取出来。此外你甚至不会说是每次用什么工具都会去盒子里面找一下，一般你总会保证伸手就够得着的地方有最近需要使用的那么几件工具，此外，你的手上可能握着正在使用的一到两件工具。你会发现，从车库到盒子到手够得着的位置再到你的手上，能够维护的工具数量越来越少，车库总是能够放上很多的工具，箱子里面可能有十几个，在身边就能够着的区域可能就只能放四五个了，手上可能拿两个工具已经是极限了。
但是不可否认，这的确会对你的工作效率产生很大的提升，其实这得益于两个定理，第一个，现在正在使用的工具在不远的未来更可能会被用到，和这个很好理解，同一个工具你可能会用很长一段时间；一个工具被用到，对于它来说比较接近，比如功能相似的一些工具在不远的将来更有可能被用到，比如各种型号的木头抛光器。
当然了，有时候你会把手上的工具换下来，然后可能会发现工具没在旁边，你可能还是会去翻盒子，如果盒子里面找不到，比如你压根儿就忘了从车库里取出来，你那么你就不得不跑到车库里面再取一遍了。而且如果还有其他人也依赖于这个车库来获取工具的话，你可能还得及时的将工具还回去，如果每次用完就还回去，你可能会在车库和工作室之间跑来跑去，很影响工作效率；但是如果每次都是到最后作品完成再还回去，可能其他人要抱怨了。聪明的你应该又看到另一个** Trade off**的影子了吧，哈哈。
让我们再回到之前的金字塔底部，我们现在好像瞥见到了两个极端，一个是金字塔的底部，另一个则是金字塔的顶端。便宜，大容量但是速度慢；速度足够快但是价格高昂且容量很小。好在金字塔不止有底部和顶端，它还有中间的部分，这也是我们的Trade off策略得以游走的空间。我们从金字塔的最底部慢慢向上攀爬探索：
  越过磁盘，首先我们可能接触的内存，虽然断电后数据就会丢失，但是其访问速度相较于磁盘已经有了质的提升，当然了，相对于磁盘，其价格更高，容量也要小很多，在1TB的固态硬盘充斥在人们的生活中时，我们通常可以操作的内存空间一般只有8G，16G等；很多的操作系统也通过虚拟内存突破了物理内存的限制，所以一般16G的内存对于绝大多数人来说都已经足够了。
  我们继续向上攀爬，此时我们会遇到称为高速缓存的东西，一般有两级，我们称之为二级缓存和以一级缓存，实际上我们的CPU尝试加载数据时，并不会直接和我们的内存打交道，我们首先会将需要访问的内存连同周边临近的内存区域先加载到二级缓存中，然后再加载一部分到一级缓存中。现在当CPU尝试去加载数据的时候，花在IO上的时间就很短了。
  然后就是寄存器了，如果你写过汇编或者有看过反编译后的Java的字节码指令，你可能会看到，我们是如何精细地一个一个存储单元的进行访问和管理的。空间很小，但是因为他们是最接近CPU的位置，它们的存在让我们的指令序列得以流畅的执行，而不必每次都花费大量时间等待IO。
  Java作为一门编程语言以及一个平台，其底层实现的时候，其实也逃不掉这些模式的束缚。只不过我们上面讨论的是硬件资源上的一些Trade off，Java内存模型是在这个基础之上又进行了一层抽象，但是其内在原理还是相通的。
本系列文章在编写时大量参考了《Java并发编程艺术》一书，原书将Java内存模型放在第三章，将Java语言中的一些特性以及实现原理，比如synchronized，volatile等关键字放在第二章。当我在准备第二篇文章的时候，老师感觉写起来不是很顺畅，因为里面有很多的概念其实是依赖于Java内存模型中的很多内容的。所以就调整了一下顺序，将JMM提到前面来写一下。
目录  Java内存模型  Java内存模型的抽象结构 指令重排序  处理器指令重排序 内存屏障 Happens-before原则 重排序  数据依赖性 as-if-serial语义 程序顺序规则 重排序对多线程的影响   顺序一致性  数据竞争与顺序一致性 顺序一致性内存模型 同步程序的顺序一致性效果 未同步程序的执行特性   Volatile的内存语义 Volatile写-读建立的happens-before关系 volatile内存语义的实现 锁的内存语义  锁的释放与获取 锁内存语义的实现 ReentrantLock-公平锁 ReentrantLock-非公平锁     Concurrent包的实现 final域的内存语义  final域的重排序规则  写final域的重排序规则 读final域的重排序规则 final域为引用类型     happens-before  Java内存模型的设计 happens-before的定义 happens-before规则   双重检查锁&amp;amp;amp;延迟初始化  基于volatile的解决方案 基于类初始化的解决方案   总结  处理器的内存模型 各种内存模型之间的关系 Java内存模型的内存可见性保证      Java内存模型  在并发编程中，我们有两个问题需要解决，当多个线程共同合作完成一个或者多个特定问题时，我们定义好线程之间如何进行通信以及如何进行同步的。通信是指线程之间如何交换信息，包括获取处理的入参输出处理的结果等等。线程之前通信的方式两种，消息传递和共享内存。</description>
    </item>
    
    <item>
      <title>Java并发机制的底层实现原理</title>
      <link>https://www.tlst.cc/post/java-concurrent-programming-03/</link>
      <pubDate>Wed, 30 Mar 2022 12:44:33 +0800</pubDate>
      
      <guid>https://www.tlst.cc/post/java-concurrent-programming-03/</guid>
      <description>无规矩不成方圆，为了获得并发编程带来的好处，我们需要定义一套严谨的控制机制。只有在这套机制的控制下，JVM才能够按照预期执行我们的字节码，只有了解这套机制，我们才能够编写出正确的代码得到正确的结果。
目录 Java并发机制的底层实现原理 Java为了方便编程人员进行实现，其暴露了很多的关键字以及Api，比如synchronized，volatile以及各种Lock。我们很容易使用这些关键字以及Api实现出一些线程安全的代码，但是我们不能止步于此，要知道在软件开发的世界里，你可以在各种各样的地方找到一些Trade off的完美诠释，同样在并发编程时，相对简单的一个实现方式通常意味着你可能牺牲了一些性能，换句话说，再花同样的时间你可能能够做的更好，在保证线程安全的情况下还能够让你的程序仍然高效的执行。。所以让我们一起看下这些特性如何使用以及其背后的原理吧。
第一个关键字 - volatile volatile，这个单词直接翻译过来是易挥发的，比如酒精，汽油等物质的挥发性。在计算机相关的属于里面，或者说是在Java语言里面，通常我们会将其翻译成易失的。我猜之所以使用这个单词是因为所有被volatile关键字标注的变量其引用的内存区域需要保证一种可见性，而JVM通过一种机制总是将最新的内容写到主存中并且也总是从主存中读取数据来实现这个，所以对于工作内存中的值，其总是很容易miss，所以才有了这么一个定义。
这一段话里面可能涉及了很多的概念，其中非常重要的有Java内存模型相关的内容，其会在后面的文章中被覆盖到，如果暂时不清楚的话可以先跳到内存模型相关的文章中了解对应的内容。
待补充</description>
    </item>
    
    <item>
      <title>并发编程的挑战</title>
      <link>https://www.tlst.cc/post/java-concurrent-programming-01/</link>
      <pubDate>Mon, 28 Mar 2022 19:06:33 +0800</pubDate>
      
      <guid>https://www.tlst.cc/post/java-concurrent-programming-01/</guid>
      <description>在讨论并发编程的挑战之前，我想要先和大家一起回顾下并发和并行的概念，这两个概念从字面上看上去其实非常的相似，很容易搞混。
首先两者中都有一个并字，我们姑且先讲并翻译为同时。
如此一来，那么并发也就是同时发生，其实更加准确一点来说应该称之为同时发生一样，什么意思呢，在单核处理器时代，分时系统通过将同一个CPU核心的执行时间进行分片，然后将时间片依次分给多个执行单元，然后在用户的感知层面达到一种每一个执行单元都在同时发生一样，比如多个应用程序同时为我们的用户提供服务。
而并行则是一个相对来说更晚一些的概念，当摩尔定律慢慢走向终结，CPU的生产厂商无法在单个的芯片上放置更多的晶体管后，他们开始转变思路，不再致力于如何在芯片上放更多的晶体管，而是为一台计算机配置更多的核心来达到同样的目的，当计算机拥有更多核心之后，我们用另外一种方式更加高效的利用计算机的算力，那就是并行。
在此之前，我们同一时间最多只能有一个执行单元被执行，其实换句话说同一时间只会有一个线程在执行，也只有一个进程在执行，即使这个进程被拆分为多个线程。但是在拥有多核的构造之后，我们不光能够继续拥有并发带给我们的好处，此外我们甚至能让同一进程的多个可执行单元，也就是多个线程在分别在多个核心上执行。总而言之，并发就是多个执行单元在同一个核心上交替执行，并行则是多个执行单元在多个核心的每一个核心上并发。
我们通过一张图片看下两者的区别，Erlang发明者Joe Armstrong通过一个咖啡机的例子生动的向大家解释了并行和并发之间的区别。
目录  并发编程的挑战  上下文切换  多线程一定快吗 我们如何减少上下文切换呢   死锁  如何避免死锁   资源限制  什么是资源限制 资源限制带来的问题 如何解决资源限制问题 在资源限制的基础上进行并发编程      并发编程的挑战  前面我们简单讲了一下并发是什么。的确，并发的目的就是为了让我们的程序运行的更快，但是并不是启动的线程越多我们的程序执行的就越快，并发时，多个可执行单元之间进行切换时会有性能开销，多个线程访问一些临界资源时也会有各种竞争条件，此外我们还会受限于硬件和软件的资源限制。
上下文切换  我们先考虑单核的情况，当我们在单核上执行多线程时，CPU通过给每个线程分配CPU时间片实现这个机制。时间片是CPU分给各个线程的执行时间，它通常比较短，比如几十毫秒，然后CPU通过不停的切换线程执行，让用户感觉多个线程是在同时执行的。
CPU的确通过时间片分配算法来实现了循环执行任务，当前任务执行一个时间片之后会切换到下一个任务，但是在切换之前，我们需要保存上一个任务的执行状态，只有这样，当下一次切换回这个任务时，我们才能够重新加载这个任务的状态并继续执行。而这个保存任务状态然后再加载并执行的过程就是上下文切换。
就比如我们在和一个人聊天的时候，突然另一个电话打进来了，我们可能需要先和当前这位聊天的小伙伴说一声抱歉，然后记住你们目前讨论的问题是什么，然后转而先去接这个临时打进来的电话，等这个电话结束，我们可能会再找到之前聊天的那个小伙伴，继续之前讨论的话题。显然，这样的切换是会影响到你们沟通的效率的，你应该经常问道或者听别人问道过，&amp;ldquo;诶，我们刚刚讲到哪儿了？&amp;quot;，同理，这样的上下文切换也会影响多线程的执行速度。
多线程一定快吗  事实上，当我们的计算量小于某一个量级时，拆分更多的线程可能最终的效率还不如让整个计算过程串行执行。因为频繁的上下文切换反而浪费了更多时间，这些时间最终随着线程数的增加反而超过了多核并行计算带来的收益。我们后面也会讲到，如何基于IO密集型还是CPU密集型的处理流程来设置恰当的线程数量来保证我们的程序有更好的性能。
我们如何减少上下文切换呢  减少上下文切换的方式有很多，比如适当控制线程数量，使用CAS算法，无锁并发编程以及协程等。
 控制线程数量 - 避免创建大量不需要的线程，比如整个任务是CPU密集型的，明明CPU已经满负荷运行了，已经以最大的计算能力处理我们的数据了，此时我们创建过多的线程，可能会适得其反，因为这时候CPU不光要忙着计算最终的结果，还需要频繁的进行上下文切换，反而拖慢了整体的效率。 CAS算法 - 全称是Compare And Swap，使用CAS算法时，我们不会尝试去获取锁，线程也就不会阻塞，也意味着我们不会失去对CPU的使用权，那么我们也就不需要进行上下文切换了。 无锁并发编程 - 多线程竞争锁时，会引起上下文切换，所以如果我们能够使用一些办法来避免使用锁或者避免锁竞争的话，就可以避免切换。比如Java 5引入的ConcurrentMap的概念以及其实现ConcurrentHashMap，就可以通过使用分段锁来提高并发度，从而避免大量的锁竞争。 协程 - 在单线程中实现多任务的调度，并在单线程里维持多个任务的切换。  死锁  当我们面对并发编程时可能遇到的竞争条件时，我们有时需要使用锁来对资源的访问进行控制，否则我们程序的执行结果就可能是错误的，花费了大量的算力最终得到的却是一个没有意义的结果。</description>
    </item>
    
  </channel>
</rss>
