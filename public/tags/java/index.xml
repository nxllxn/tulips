<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Java on Tulips</title>
    <link>https://www.tlst.cc/tags/java/</link>
    <description>Recent content in Java on Tulips</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>Copyright @ 2022 郁金香啊</copyright>
    <lastBuildDate>Wed, 30 Mar 2022 12:44:33 +0800</lastBuildDate><atom:link href="https://www.tlst.cc/tags/java/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Java内存模型</title>
      <link>https://www.tlst.cc/post/java-concurrent-programming-02/</link>
      <pubDate>Wed, 30 Mar 2022 12:44:33 +0800</pubDate>
      
      <guid>https://www.tlst.cc/post/java-concurrent-programming-02/</guid>
      <description>在我们学习操作系统时，有一个章节会讲数据存储相关的的内容，那个经典的金字塔模型。哈哈，又一个Trade off 的完美诠释。这个金字塔模型中指出，通常我们可以很容易的获得一些存储设备，其数据容量大，成本低廉，但是访问速度较慢，比如我们常见的磁盘，甚至可以扩展到网盘。我们还有一些存储介质，其容量非常小，小到当我们使用这类存储介质所能提供的空间时，不得不小心翼翼，而且此类介质一般价格十分高昂，但是由于这类存储介质相对来说更加靠近我们的计算单元，所以其访问速度非常非常快，几乎可以忽略不记，比如我们的寄存器。
在我们讨论这个抽象的金字塔概念之前，让我们先看一个生活中比较常见的例子。比如你是一个木匠，你准备做点科研，造个什么木制品出来，这个作品相当精妙，以至于需要十几种甚至更多的工具来帮助你完成，假定这些工具目前放在你的车库。
现实中，当你用工具的时候你并不会总是去车库找工具，你可能会先找一个小盒子将你认为在不远的将来你可能会用到的工具先一次性取出来。此外你甚至不会说是每次用什么工具都会去盒子里面找一下，一般你总会保证伸手就够得着的地方有最近需要使用的那么几件工具，此外，你的手上可能握着正在使用的一到两件工具。你会发现，从车库到盒子到手够得着的位置再到你的手上，能够维护的工具数量越来越少，车库总是能够放上很多的工具，箱子里面可能有十几个，在身边就能够着的区域可能就只能放四五个了，手上可能拿两个工具已经是极限了。
但是不可否认，这的确会对你的工作效率产生很大的提升，其实这得益于两个定理，第一个，现在被使用的工具在不远的未来更可能会被用到，和这个很好理解，同一个工具你可能会用很长一段时间；一个工具被用到，与它相对来说比较接近，比如功能相似的一些工具在不远的将来更有可能被用到，比如各种型号的木头抛光器。
当然了，有时候你会把手上的工具换下来，然后可能会发现工具没在旁边，你可能还是会去翻盒子，如果盒子里面找不到，比如你压根儿就忘了从车库里取出来，你那么你就不得不跑到车库里面再取一遍了。而且如果还有其他人也依赖于这个车库来获取工具的话，你可能还得及时的将工具还回去，如果每次用完就还回去，你可能会在车库和工作室之间跑来跑去，很影响工作效率；但是如果每次都是到最后作品完成再还回去，可能其他人要抱怨了。聪明的你应该又看到另一个** Trade off**的影子了吧，哈哈。
让我们再回到之前的金字塔底部，我们现在好像瞥见到了两个极端，一个是金字塔的底部，另一个则是金字塔的顶端。便宜，大容量但是速度慢；速度足够快但是价格高昂且容量很小。好在金字塔不止有底部和顶端，它还有中间的部分，这也是我们的Trade off策略得以游走的空间。我们从金字塔的最底部慢慢向上攀爬探索：
     Java作为一门编程语言以及一个平台，其底层实现的时候，其实也逃不掉这些模式的束缚。只不过我们上面讨论的是硬件资源上的一些Trade off，Java内存模型是在这个基础之上又进行了一层抽象，但是其内在原理还是相通的。
本系列文章在编写时大量参考了《Java并发编程艺术》一书，原书将Java内存模型放在第三章，将Java语言中的一些特性以及实现原理，比如synchronized，volatile等关键字放在第二章。当我在准备第二篇文章的时候，老师感觉写起来不是很顺畅，因为里面有很多的概念其实是依赖于Java内存模型中的很多内容的。所以就调整了一下顺序，将JMM提到前面来写一下。
目录 Java内存模型 在并发编程中，我们有两个问题需要解决，当多个线程共同合作完成一个或者多个特定问题时，我们定义好线程之间如何进行通信以及如何进行同步的。通信是指线程之间如何交换信息，包括获取处理的入参输出处理的结果等等。线程之前通信的方式两种，消息传递和共享内存。
其中消息传递这种模式，在一些移动开发时可能会遇到，比如Android开发时，我们会有一个UI线程，还会有一些子线程，子线程一般用于去获取数据，比如调用一个接口获取数据，而主线程我们也叫UI线程的话，顾名思义，主要工作是进行UI的渲染。我们不希望在UI线程里面去进行IO操作，这样可能会导致线程阻塞相当长的时间，界面就无法进行组件的渲染，更无法响应用户的操作，这对用户来说是完全不可接受的。真因为这样我们才会在子线程中做IO相关的操作。当子线程成功加载好数据之后，我们会使用一种消息传递的机制来通知UI线程，我已经成功获取了数据，你可以拿最新的数据进行渲染了。
相比于消息传递，共享内存这种方式来进行数据共享就相对来说要常见的多了。多个线程往往通过读写内存中的公共状态来进行隐式通信。
不管是消息传递还是共享内存，我们都需要做一件事情，那就是指定一个数据的同步访问机制，避免由于大家同时对数据进行读写导致最终获得一个错误的结果。在消息传递模型中，消息的接收一定在消息的发送之后，因此共享数据的访问一直都是串行的，同步是隐式进行的。在共享内存模型中，数据访问的顺序无法预见，所以程序员必须显式的指定某个方法或者某个代码片段需要在线程之间互斥的执行，同步是显式进行的。
Java并发采用的是共享内存模型，默认情况下，Java线程之前的通信总是隐式进行的，整个通信过程对程序员完全透明，如果程序员没有意识到这个并且不手动加以控制就不可能得到正确的结果。
Java内存模型的抽象结构 Java定义了一套抽象的模型来控制线程间在进行内存共享时共享内容在各个线程之间的可见性，这个模型决定了一个线程对一个共享变量的写入何时对另一个线程可见。
从抽象角度来看，JMM定义了主存和线程工作内存之间的关系。线程间的共享变量存储在主存中，而每一个线程都有一个线程私有的本地内存，我们也称之为工作内存，工作内存中存储了该线程读写共享变量的副本。
工作内存是JMM的一个抽象概念，并不是真实存在的，它涵盖了寄存器，高速缓存，写缓冲区以及其他硬件以及编译器优化。
 假定两个线程A，B同时访问一个共享变量var，根据JMM，除了主存中的数据var，线程A，B在工作内存中还存在两个副本，var-A，var-B。当线程A，B对共享变量进行读取或者修改时，他们直接操作工作内存中的副本。
那么当其中一个线程对数据进行了更新时，如果另一个线程想要读取到最新的结果，我们就必须将更新过后的值刷新到主内存中去，然后另一个线程在读取数据时，不能直接从工作内存的副本中读取过期的数据，其必须直接从主存中加载最新的结果。而JMM正是通过控制朱迅与每个线程的本地内存之间的交互，来为Java程序员提供内存可见性的保证。
指令重排序 在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序，其分为以下三种类型：
 编译器优化的指令重排序 - 编译器在不改变单线程语义的情况下，可以重新安排语句的执行顺序 指令级并行重排序 - 如果多条指令之间不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序 - 由于处理器使用缓存和读写缓冲区，使得加载和存储操作看上去可能在乱序执行。  重排序能够提升性能，但是也可能带来一些内存可见性问题，比如在单例模式实现的例子中就会讲到，指令重排序是如何导致可见性问题并使用volatile关键字来解决它的。
对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序。对于指令级并行重排序，JMM的处理器重排序规则会要求Java编译器在生成指令序列时插入特定的内存屏障指令，通过这种指令来禁止特定类型的指令级并行重排序。
JMM属于语言级的内存模型，它确保在不同比那一起和不同的处理器平台之上，通过禁特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。
处理器指令重排序 现代的处理器使用写缓冲区临时保存向内存写入的数据，写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停下来等待向内存写入数据而产生的延迟。同时，通过以批处理的形式刷新写缓冲区，以及合并写缓冲区对同一内存地址的多次写，减少对内存总线的占用。</description>
    </item>
    
    <item>
      <title>Java并发机制的底层实现原理</title>
      <link>https://www.tlst.cc/post/java-concurrent-programming-03/</link>
      <pubDate>Wed, 30 Mar 2022 12:44:33 +0800</pubDate>
      
      <guid>https://www.tlst.cc/post/java-concurrent-programming-03/</guid>
      <description>无规矩不成方圆，为了获得并发编程带来的好处，我们需要定义一套严谨的控制机制。只有在这套机制的控制下，JVM才能够按照预期执行我们的字节码，只有了解这套机制，我们才能够编写出正确的代码得到正确的结果。
目录 Java并发机制的底层实现原理 Java为了方便编程人员进行实现，其暴露了很多的关键字以及Api，比如synchronized，volatile以及各种Lock。我们很容易使用这些关键字以及Api实现出一些线程安全的代码，但是我们不能止步于此，要知道在软件开发的世界里，你可以在各种各样的地方找到一些Trade off的完美诠释，同样在并发编程时，相对简单的一个实现方式通常意味着你可能牺牲了一些性能，换句话说，再花同样的时间你可能能够做的更好，在保证线程安全的情况下还能够让你的程序仍然高效的执行。。所以让我们一起看下这些特性如何使用以及其背后的原理吧。
第一个关键字 - volatile volatile，这个单词直接翻译过来是易挥发的，比如酒精，汽油等物质的挥发性。在计算机相关的属于里面，或者说是在Java语言里面，通常我们会将其翻译成易失的。我猜之所以使用这个单词是因为所有被volatile关键字标注的变量其引用的内存区域需要保证一种可见性，而JVM通过一种机制总是将最新的内容写到主存中并且也总是从主存中读取数据来实现这个，所以对于工作内存中的值，其总是很容易miss，所以才有了这么一个定义。
这一段话里面可能涉及了很多的概念，其中非常重要的有Java内存模型相关的内容，其会在后面的文章中被覆盖到，如果暂时不清楚的话可以先跳到内存模型相关的文章中了解对应的内容。
待补充</description>
    </item>
    
    <item>
      <title>并发编程的挑战</title>
      <link>https://www.tlst.cc/post/java-concurrent-programming-01/</link>
      <pubDate>Mon, 28 Mar 2022 19:06:33 +0800</pubDate>
      
      <guid>https://www.tlst.cc/post/java-concurrent-programming-01/</guid>
      <description>在讨论并发编程的挑战之前，我想要先和大家一起回顾下并发和并行的概念，这两个概念从字面上看上去其实非常的相似，很容易搞混。
首先两者中都有一个并字，我们姑且先讲并翻译为同时。
如此一来，那么并发也就是同时发生，其实更加准确一点来说应该称之为同时发生一样，什么意思呢，在单核处理器时代，分时系统通过将同一个CPU核心的执行时间进行分片，然后将时间片依次分给多个执行单元，然后在用户的感知层面达到一种每一个执行单元都在同时发生一样，比如多个应用程序同时为我们的用户提供服务。
而并行则是一个相对来说更晚一些的概念，当摩尔定律慢慢走向终结，CPU的生产厂商无法在单个的芯片上放置更多的晶体管后，他们开始转变思路，不再致力于如何在芯片上放更多的晶体管，而是为一台计算机配置更多的核心来达到同样的目的，当计算机拥有更多核心之后，我们用另外一种方式更加高效的利用计算机的算力，那就是并行。
在此之前，我们同一时间最多只能有一个执行单元被执行，其实换句话说同一时间只会有一个线程在执行，也只有一个进程在执行，即使这个进程被拆分为多个线程。但是在拥有多核的构造之后，我们不光能够继续拥有并发带给我们的好处，此外我们甚至能让同一进程的多个可执行单元，也就是多个线程在分别在多个核心上执行。总而言之，并发就是多个执行单元在同一个核心上交替执行，并行则是多个执行单元在多个核心的每一个核心上并发。
我们通过一张图片看下两者的区别，Erlang发明者Joe Armstrong通过一个咖啡机的例子生动的向大家解释了并行和并发之间的区别。
目录  并发编程的挑战  上下文切换  多线程一定快吗 我们如何减少上下文切换呢   死锁  如何避免死锁   资源限制  什么是资源限制 资源限制带来的问题 如何解决资源限制问题 在资源限制的基础上进行并发编程      并发编程的挑战  前面我们简单讲了一下并发是什么。的确，并发的目的就是为了让我们的程序运行的更快，但是并不是启动的线程越多我们的程序执行的就越快，并发时，多个可执行单元之间进行切换时会有性能开销，多个线程访问一些临界资源时也会有各种竞争条件，此外我们还会受限于硬件和软件的资源限制。
上下文切换  我们先考虑单核的情况，当我们在单核上执行多线程时，CPU通过给每个线程分配CPU时间片实现这个机制。时间片是CPU分给各个线程的执行时间，它通常比较短，比如几十毫秒，然后CPU通过不停的切换线程执行，让用户感觉多个线程是在同时执行的。
CPU的确通过时间片分配算法来实现了循环执行任务，当前任务执行一个时间片之后会切换到下一个任务，但是在切换之前，我们需要保存上一个任务的执行状态，只有这样，当下一次切换回这个任务时，我们才能够重新加载这个任务的状态并继续执行。而这个保存任务状态然后再加载并执行的过程就是上下文切换。
就比如我们在和一个人聊天的时候，突然另一个电话打进来了，我们可能需要先和当前这位聊天的小伙伴说一声抱歉，然后记住你们目前讨论的问题是什么，然后转而先去接这个临时打进来的电话，等这个电话结束，我们可能会再找到之前聊天的那个小伙伴，继续之前讨论的话题。显然，这样的切换是会影响到你们沟通的效率的，你应该经常问道或者听别人问道过，&amp;ldquo;诶，我们刚刚讲到哪儿了？&amp;quot;，同理，这样的上下文切换也会影响多线程的执行速度。
多线程一定快吗  事实上，当我们的计算量小于某一个量级时，拆分更多的线程可能最终的效率还不如让整个计算过程串行执行。因为频繁的上下文切换反而浪费了更多时间，这些时间最终随着线程数的增加反而超过了多核并行计算带来的收益。我们后面也会讲到，如何基于IO密集型还是CPU密集型的处理流程来设置恰当的线程数量来保证我们的程序有更好的性能。
我们如何减少上下文切换呢  减少上下文切换的方式有很多，比如适当控制线程数量，使用CAS算法，无锁并发编程以及协程等。
 控制线程数量 - 避免创建大量不需要的线程，比如整个任务是CPU密集型的，明明CPU已经满负荷运行了，已经以最大的计算能力处理我们的数据了，此时我们创建过多的线程，可能会适得其反，因为这时候CPU不光要忙着计算最终的结果，还需要频繁的进行上下文切换，反而拖慢了整体的效率。 CAS算法 - 全称是Compare And Swap，使用CAS算法时，我们不会尝试去获取锁，线程也就不会阻塞，也意味着我们不会失去对CPU的使用权，那么我们也就不需要进行上下文切换了。 无锁并发编程 - 多线程竞争锁时，会引起上下文切换，所以如果我们能够使用一些办法来避免使用锁或者避免锁竞争的话，就可以避免切换。比如Java 5引入的ConcurrentMap的概念以及其实现ConcurrentHashMap，就可以通过使用分段锁来提高并发度，从而避免大量的锁竞争。 协程 - 在单线程中实现多任务的调度，并在单线程里维持多个任务的切换。  死锁  当我们面对并发编程时可能遇到的竞争条件时，我们有时需要使用锁来对资源的访问进行控制，否则我们程序的执行结果就可能是错误的，花费了大量的算力最终得到的却是一个没有意义的结果。</description>
    </item>
    
  </channel>
</rss>
